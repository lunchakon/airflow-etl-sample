# Apache Airflow ETL â€” API â†’ PostgreSQL Pipeline
A simple **ETL (Extract, Transform, Load)** project built with **Apache Airflow** running in **Docker Compose**.  
This example demonstrates how to orchestrate daily workflows that extract data from an API, transform it using Python & Pandas, and load the results into PostgreSQL.

## ðŸš€ Features

- ðŸ§  **Fully containerized** â€” no local Airflow installation required  
- ðŸ” **Daily scheduled DAG** using Airflow Scheduler  
- ðŸ§© **ETL steps:**  
  1. Extract JSON from a REST API  
  2. Transform using Pandas  
  3. Load into PostgreSQL  
- ðŸ“Š **Web UI** to monitor and trigger DAGs  
- ðŸ› ï¸ Extendable for ML, analytics, or data warehouse pipelines


## ðŸ—ï¸ Architecture Overview
```
graph TD;
    A[API Source] --> B[Airflow Extract Task];
    B --> C[Transform Task (Pandas)];
    C --> D[Load Task (PostgreSQL)];
    D --> E[Data Warehouse / Reports];
	
	
airflow-etl/
â”œâ”€â”€ dags/
â”‚   â””â”€â”€ api_to_postgres_dag.py     # DAG definition
â”œâ”€â”€ logs/                          # Airflow logs
â”œâ”€â”€ docker-compose.yaml            # Docker environment
â”œâ”€â”€ requirements.txt               # Extra Python libs for DAG
â””â”€â”€ README.md                      # Documentation
